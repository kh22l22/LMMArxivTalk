# LMMArxivTalk
최신 LLM 관련 논문 스터디. 항상 오후에 진행.

LLM, NLG, Dialogue, Reinforcement learning, Distillation, Efficient, Sentence similarity, multiple tasks, multimodal, Stable diffusion, TTS, Text-To-Video, All-To-All ETC...

삼성, SKT, LG, KT 등 탑티어 연구자들, 예비 창업자, 튜닙, ForUS.Ai 대표 및 공동 대표, 아이비리그 급 졸업생, 석학, 교수 등 A급 인재들이 LLM 최신 논문 스터디 진행.

매주 수요일 7시반. 사전 학습 없이 논문 읽기 20분, 토론 40분. 한 번에 1 ~ 10개 논문, 강의 등 진행. 주제 논문 선정은 자유. 탑티어 학회 논문 및 프로젝트 제작 예정.

되는 날만 중간에 들어와서 중간에 나가도 무관. 모든 규칙은 협의 가능.

## 규칙
1. 영어 금지.
2. 외국인 금지.
3. 1주일에 논문 2개 이상.
4. 되는 사람은 10개 이상.
5. 최대 20분 현장에서 논문 읽기.
6. 최대 40분 토론.
7. 1시간 스터디 시 바로 나가도 됨.
8. 자유롭게.
9. 모두연 규칙 붙여넣기.
10. 다들 대단한 분들이니 질문 많이.
11. 공유 자주.
12. 각자 더 뛰어난게 있다는 것을 인지.
13. 겸손하기 노력하기 잘하기.
14. 잠수 금지.

## 진행 사항 + 예정
2023-02-16 11:30 ~ 12:45 염기웅, 강수진, 고현웅
- [GPT Understands, To](https://arxiv.org/pdf/2103.10385.pdf)
- [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/abs/2110.07602.pdf)
- [Do Prompt-Based Models Really Understand the Meaning of their Prompts?](https://arxiv.org/pdf/2109.01247.pdf)

2023-02-18 7:30 ~ 8:30 염기웅, 박상준, 강수진, 
- [∞-former: Infinite Memory Transformer](https://arxiv.org/abs/2109.00301)

2023-02-19 11:30 ~ 12:30 염기웅, 박상준, 강수진, 김찬란, 
- [Improving language models by retrieving from trillions of tokens](https://arxiv.org/pdf/2112.04426.pdf)

2023-02-22 7:30 ~ 8:30 염기웅, 박상준, 강수진, 고현웅, 이현제
- [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/pdf/2301.12597v1.pdf)

## 후보
앞으로 할만한 논문, 코드, 강의 등.

### papaer
- [Improving language models by retrieving from trillions of tokens](https://arxiv.org/pdf/2112.04426.pdf)
- [FLAN: Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652.pdf)
- [T0: Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207.pdf)
- [The Flan Collection: Designing Data and Methods for Effective Instruction Tuning](https://arxiv.org/abs/2301.13688.pdf)
- [The Wisdom of Hindsight Makes Language Models Better Instruction Followers](https://arxiv.org/abs/2302.05206.pdf)
- [Exploring the Benefits of Training Expert Language Models over Instruction Tuning](https://paperswithcode.com/paper/exploring-the-benefits-of-training-expert.pdf)
- [Unsupervised Imputation of Non-ignorably Missing Data Using Importance-Weighted Autoencoders](https://arxiv.org/abs/2101.07357)
- [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)
- [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073.pdf)
- [Deep reinforcement learning from human preferences](https://arxiv.org/abs/1706.03741.pdf)
- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961.pdf)
- [Large Language Models with Controllable Working Memory](https://arxiv.org/abs/2211.05110.pdf)
- [Do Prompt-Based Models Really Understand the Meaning of their Prompts?](https://arxiv.org/abs/2109.01247.pdf)
- [Muse: Text-To-Image Generation via Masked Generative Transformers](https://arxiv.org/pdf/2301.00704v1.pdf)
- [Structure and Content-Guided Video Synthesis with Diffusion Models](https://arxiv.org/abs/2302.03011.pdf)
- [Generative Pretraining from Pixels](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf)
- [A hunt for the Snark: Annotator Diversity in Data Practices](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/c5dbc2c146b7443447d43a344b4a22a359b32b16.pdf)
- [Accurate global machine learning force fields for molecules with hundreds of atoms](https://www.science.org/doi/full/10.1126/sciadv.adf0873)
- [Algorithms with More Granular Differential Privacy Guarantees](https://arxiv.org/pdf/2209.04053.pdf)
- [Anomaly Clustering: Grouping Images into Coherent Clusters of Anomaly Types](https://research.google/pubs/pub51881/)
- [Are we cobblers without shoes? Making Computer Science data FAIR](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/962979aa7d369692aa1919dcb517a5e5a3d0fa66.pdf)
- [Code Generation for In-Place Stencils](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/26cdf8c85c428bbff96ff6e7a07fe313e9844d68.pdf)
- [Creating, Calibrating, and Validating Large-Scale Microscopic Traffic Simulation](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/0e0284222cf7d2d690f1b277b970de8e3f61294a.pdf)
- [Increasing Impact of Mobile Health Programs: SAHELI for Maternal and Child Care](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/4e67c21de7542a2eb61edd37ece44c86928e00b1.pdf)
- [Designing Responsible AI: Adaptations of UX Practice to Meet Responsible AI Challenges](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/4045f0e0e61d89b6b1eacad4b861e86631d5e660.pdf)
- [Developer Productivity for Humans: A Human-Centered Approach to Developer Productivity](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9994260)
- [Development of a Machine Learning Model for Sonographic Assessment of Gestational Age](https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2800007)
- [Drug Design on Quantum Computers](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/20b69430c8fd64f170d5c7ba0a0535aa51e70ab6.pdf)
- [Estimates of broadband upwelling irradiance from GOES-16 ABI](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/146e5813afdf4f8ba02d18a801c6ed06de27d7d5.pdf)
- [Information Processing and Management](https://reader.elsevier.com/reader/sd/pii/S0306457322003508?token=D58C3F3F1C12E28D51A5515FA2F3608D25FB798EBDEE89D440A941BFCFAC8872866F56B9D4B863D69728CCCB26963764&originRegion=us-east-1&originCreation=20230216172152)
- [Flake Aware Culprit Finding](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/a422ac172dbb3c6521f2bc6c83d363695f4e911c.pdf)
- [Flexible Budgets in Restless Bandits: A Primal-Dual Algorithm for Efficient Budget Allocation](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/00074e5dacad1fe2e7f03f65274993fcfbc860bd.pdf)
- [Helpful Neighbors: Leveraging Neighbors in Geographic Feature Pronunciation](https://watermark.silverchair.com/tacl_a_00535.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAr0wggK5BgkqhkiG9w0BBwagggKqMIICpgIBADCCAp8GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMGaBmgqnePMpUEr_1AgEQgIICcHADbjfmiT9rEwbAOxm_3_0F7rZ2w_ETcJECYlEEpAHZUqPFKMP6eae59zDGkfZVaF3pq7w2JGFl-PJqQyT_fdQxIadVrVhW-xsibjoGYmAnciIFFIedkpSKJqXP2k4KG-lrRck9cEprUnptSHqq_VHWZFCETv1b9EuexXPfjHGEYDVCFEHER2nfwGzp4E7iMMvlOm53M3gU_WUmXcJOZcAnOgmYCyNh01R7f3vzI4Y1zKz9lhGNTL86mU-wFv-8aUWWbtIFLoiCgGaAgmp3m5N-vmefhqL4UtutrkAfTwq0xxg6VmEoSpBk52mIBusp07qxFA8s0wXFu3mBkJLJAVFqLtBJsyl2jic59TmHY4BDt7An6WcozDInnxILNbClDQAqVIh6OLNOJ4r9MXFUAyTrX3lbqN52SlygwV4K3z_ZFdGM7ndaWuJg5Ou2UdXfnaFrSjyvcNRutcUaupT7DGcXV3L7wb7b-XKA4OoAdSS8x1s-hhNW4l-I_iuiTx5vRAsVY0EyhOR2EPajiLKiEVN7SDEZ9ihAtef312FBsqu9xK10m14chdHdxvuIrwtuBwcVaj1rMPIzRPLU_l3Hxfa2x735pDjHZhDxzm2rEyXtlOXY-VIiv2gXD539gB1KFQ3VCjVnvOsesXlMue9a0LPPU6Tyb7wpo43ERNs8U5s5BRmomjOsUMyeM_Zp47O94WNyMo6V_F2bjhEZJq-lJZxtyqGcN6QFXiSdGoMIG5xQasrVUButsE3tVJELEjZU_j5rRKt8iXYPvRXsCNHZWG0_ezt_NsHM6H9xgz6vTFtezIGiLmpYZIBcVcU51Iq5bw)
- [High-Performance GPU-to-CPU Transpilation and Optimization via High-Level Parallel Constructs](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/ca9caaef5f92cb1c1088897a9654a79ccdcb2bd4.pdf)
- [Helpful Neighbors: Leveraging Neighbors in Geographic Feature Pronunciation](https://watermark.silverchair.com/tacl_a_00535.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAAr0wggK5BgkqhkiG9w0BBwagggKqMIICpgIBADCCAp8GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQM9k6JNPdG5DftjzF0AgEQgIICcAmosEi5f1ji5KSTr9mz9ZwOEdbRU73_HgVTuXYzZyu500yymmCBEL9V1rg0Dv6ynhoWfTZJIozQ8Qd30uTpLwf5N167hksw3DLEAIsOMG4zoYFts1uazpEn42BVoQANpM-xj9ruJM5YQuYWscLrXzmCGAI93pQSxPlkjiwPQtsUlX713I5Qn_QDaV8_sGTMu0yw_Dxbz9p5dtFy_wovYDhzCB4W7yQlEaVhFUvk04A3PEfVmIi2c0sctYptcz8QDEnPI-kGpD_mplO65b0QHmYYBWriZWc13d4M6RyUWAgrpMQinmZtFRSxYjBof-BoqnlLEMpxfMECrFRTK3Jx3sKfWPpEfye_VpcJ4b-PqDpCHuMnjSL2jqmcRN5Fp9vhyjECs7HMepK618KC8NAJRN4_YbQnyOm-UT5MzaL72aQsaChkr1xZi9JOUe1xvqdjazEH9y9VgvW7cM8XCZZVLum87s2zsx8qzjqPMsfZLRoc8HsB9Tu-dvuxGPQK3gKetGRQjatjbKn4a3CSWhAij5ayG66Aa4AaYi-33iqG4EcBH6WLQG9JouFiojC1Pkj03O0evnvXP1zy11CZ_3w9QjScEfXdUh3no0dukyw5pnsrlxDWdGhZZFQx1NfGcYDOi6luzEpAijwKIVm7FogdhVg_hD63JoddNI9y7iTESYBvkBwt249qyhnpJ2Z5osFuipKGyOEYog0DYh1jxY-BIu1CY-vb9uLr1Jud7zCXkIwxfYswmLXiaJJQQo3v6Rle5Wek3MxP4CN4X4mwtQrrFJFKwddfMa6Ki5ViOTS-ckW_zLoNaORYu8UKfz_2fPjxNg)
- [Infrastructuring Care: How Trans and Non-Binary People Meet Health and Well-Being Needs through Technology](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/98030d5f96d6f383c90e40fcbf79425919f178d3.pdf)
- [KwikBucks: Correlation Clustering with Cheap-Weak and Expensive-Strong Signals](https://openreview.net/pdf?id=p0JSSa1AuV)
- [Learning to Bid in Contextual First Price Auctions](https://arxiv.org/pdf/2109.03173.pdf)
- [Machine Learning for Healthcare: A Bibliometric Study of Contributions from Africa](https://www.preprints.org/manuscript/202302.0010/v1)
- [Scalable Decision-Focused Learning in Restless Multi-Armed Bandits with Application to Maternal and Child Health](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/c394143cc1cb501f710a304f15983e70ed22fa8a.pdf)
- [Robust Planning over Restless Groups: Engagement Interventions for a Large-Scale Maternal Telehealth Program](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/c3043326e08d5775bc766ab494254e02560cfbe0.pdf)
- [Recitation-Augmented Language Models](https://arxiv.org/pdf/2210.01296.pdf)
- [RL4ReAl: Reinforcement Learning for Register Allocation](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/cfad891e4eee1a4c38f55bff5117b3ba7aa65392.pdf)
- [Quantum Simulation of Exact Electron Dynamics can be more Efficient than Classical Mean-Field Methods](https://research.google/pubs/pub52012/)
- [Quantum simulation of exact electron dynamics can be more efficient than classical mean-field methods](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/231543ccbbbd24d725563e79c01743fbb4eb06e0.pdf)
- [Propeller: A Profile Guided, Relinking Optimizer for Warehouse-Scale Applications](https://dl.acm.org/doi/10.1145/3575693.3575727)
- [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/pdf/2205.11916.pdf%EC%97%90%EC%84%9C)

### github
- [DistilKoBiLSTM](https://github.com/gyunggyung/DistilKoBiLSTM)
- [Donut 🍩 : Document Understanding Transformer](https://github.com/clovaai/donut)
- [The RWKV Language Model (and my LM tricks)](https://github.com/BlinkDL/RWKV-LM)

### youtube
- [Study Playlist](https://youtube.com/playlist?list=PLsmJteXozP3oHVB5TCrXEcrfQnInMxkoT)

- [Improving Language Models by Retrieving from Trillions of Tokens | NLP Journal Club](https://youtu.be/-cnujwUDseU)
- [ECMLPKDD2021: WuDao: Pretrain the World, Keynote speaker talk by Jie Tang](https://youtu.be/D71zxGRhuxE)
- [StrictlyVC in conversation with Sam Altman, part two (OpenAI)](https://youtu.be/ebjkD1Om4uw)
- [Are Bigger Language Models Better? | DeepMind Gopher and RETRO](https://youtu.be/IaltsI1BCro)
- [The Illustrated Retrieval Transformer](https://youtu.be/sMPq4cVS4kg)

### deepmind
- [Deepmind: Improving language models by retrieving from trillions of tokens](https://www.deepmind.com/publications/improving-language-models-by-retrieving-from-trillions-of-tokens)
- [Deepmind: Building safer dialogue agents](https://www.deepmind.com/blog/building-safer-dialogue-agents)
- [Deepmind: Competitive programming with AlphaCode](https://www.deepmind.com/blog/competitive-programming-with-alphacode)
- [Deepmind: Mastering Stratego, the classic game of imperfect information](https://www.deepmind.com/blog/mastering-stratego-the-classic-game-of-imperfect-information)
- [Deepmind: DeepMind’s latest research at NeurIPS 2022](https://www.deepmind.com/blog/deepminds-latest-research-at-neurips-2022)
- [Deepmind: Building interactive agents in video game worlds](https://www.deepmind.com/blog/building-interactive-agents-in-video-game-worlds)
- [Deepmind: Discovering novel algorithms with AlphaTensor](https://www.deepmind.com/blog/discovering-novel-algorithms-with-alphatensor)
- [Deepmind: AlphaFold reveals the structure of the protein universe](https://www.deepmind.com/blog/alphafold-reveals-the-structure-of-the-protein-universe)
- [Deepmind: Tackling multiple tasks with a single visual language model](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model)
- [Deepmind: Exploring the beauty of pure mathematics in novel ways](https://www.deepmind.com/blog/exploring-the-beauty-of-pure-mathematics-in-novel-ways)
- [Deepmind: Nowcasting the next hour of rain](https://www.deepmind.com/blog/nowcasting-the-next-hour-of-rain)
- [Deepmind: Putting the power of AlphaFold into the world’s hands](https://www.deepmind.com/blog/putting-the-power-of-alphafold-into-the-worlds-hands)

### google
- [Google Research: Deciphering clinical abbreviations with privacy protecting ML](https://ai.googleblog.com/2023/01/deciphering-clinical-abbreviations-with.html)
- [Google Research: Google Research, 2022 & beyond: Language, vision and generative models](https://ai.googleblog.com/2023/01/google-research-2022-beyond-language.html)
- [Google Research: Google Research, 2022 & beyond: Responsible AI](https://ai.googleblog.com/2023/01/google-research-2022-beyond-responsible.html)
- [Google Research: Learning with queried hints](https://ai.googleblog.com/2023/01/learning-with-queried-hints.html)
- [Google Research: Open Source Vizier: Towards reliable and flexible hyperparameter and blackbox optimization](https://ai.googleblog.com/2023/02/open-source-vizier-towards-reliable-and.html)
- [Google Research: Google Research, 2022 & beyond: ML & computer systems](https://ai.googleblog.com/2023/02/google-research-2022-beyond-ml-computer.html)
- [Google Research: Real-time tracking of wildfire boundaries using satellite imagery](https://ai.googleblog.com/2023/02/real-time-tracking-of-wildfire.html)
- [Google Research: Breaching the 2 LMP Approximation Barrier for Facility Location with Applications to k-Median](https://research.google/pubs/pub51938/)
- [Google Research: Chimane-Mosetén](https://research.google/pubs/pub52097/)
- [Google Research: Differentially Private All-Pairs Shortest Path Distances: Improved Algorithms and Lower Bounds](https://research.google/pubs/pub51926/)
- [Google Research: Differentially Private Fair Division](https://research.google/pubs/pub51931/)
- [Google Research: DiffQG: Generating Questions on Paired Sentences](https://research.google/pubs/pub52078/)
- [Google Research: Assessment of Security Defense of Native Programs Against Software Faults](https://link.springer.com/chapter/10.1007/978-3-031-02063-6_5)
- [Google Research: Adaptive mixing of auxiliary losses in supervised learning](https://research.google/pubs/pub51874/)

### openai
- [OpenAI: Multimodal Neurons in Artificial Neural Networks](https://openai.com/blog/multimodal-neurons/)
- [OpenAI: DALL·E: Creating Images from Text](https://openai.com/blog/dall-e/)
- [OpenAI: CLIP: Connecting Text and Images](https://openai.com/blog/clip/)
- [OpenAI: Image GPT](https://openai.com/blog/image-gpt/)
- [OpenAI: Jukebox](https://openai.com/blog/jukebox/)
- [OpenAI: Solving Rubik’s Cube with a Robot Hand](https://openai.com/blog/solving-rubiks-cube/)
- [OpenAI: Multimodal Neurons in Artificial Neural Networks](https://openai.com/blog/multimodal-neurons/)
- [OpenAI: CLIP: Connecting Text and Images](https://openai.com/blog/clip/)
- [OpenAI: Image GPT](https://openai.com/blog/image-gpt/)
- [OpenAI: MuseNet](https://openai.com/blog/musenet/)
- [OpenAI: Emergent Tool Use from Multi-Agent Interaction](https://openai.com/blog/emergent-tool-use/)

### other
- [CS224U: Natural Language Understandin](http://web.stanford.edu/class/cs224u/)
- [Gen-1: The Next Step Forward for Generative AI](https://research.runwayml.com/gen1)
- [DIFF-SVC FOR VOCAL SYNTH USERS](https://docs.google.com/document/u/0/d/1nA3PfQ-BooUpjCYErU-BHYvg2_NazAYJ0mvvmcjG40o/mobilebasic)
- [Chat GPT detector by ZeroGPT: detect OpenAI text](https://www.zerogpt.com/)

## 참여 인원 소개
1. 염기웅: 저는 여러분을 모으고 프로메우스와 바드의 꿈이라는 책을 쓰는 염기웅입니다. LLM Dialogue Distillation 에 관심과 경험이 있습니다! 경량화 Mlops 서빙 멀티모달 멀티태스크 모델에도 관심이 있습니다. newhiwoong@gmail.com https://github.com/gyunggyung
2. 강수진: 
3. 고현웅: 
4. 박상준: 스타트업 ML 엔지니어로 일하다 휴직 후 성균관대 학부생으로 NLP 연구를 하고 있습니다.
5. 김찬란: 
6. 이현제: 삼성SDS에서 자연어처리를 연구하고 있습니다. instruction finetuning, sentence representation 쪽에 관심이 있습니다. oglee815@gmail.com 
7. 김기현: SKTelecom에서 LLM을 활용한 대화모델링을 연구/개발 하고 있습니다 -- [Github link](https://github.com/kh-kim/)
8. 이광희: VIVE STUDIOS에서 CTO를 맡고 있는 이광희 입니다. Generative  AI,  LLM등을 활용하여 Virtual  Production, Virtual  Human을 포함한 컨텐츠 저작 도구 서비스화에 관심이 있습니다. lkwanghee@gmail.com

## Google Scholar
Google Scholar에서 추천 받은 것들

### Richard Socher - 새로운 관련 연구
[PDF] Stabilized In-Context Learning with Pre-trained Language Models for Few Shot Dialogue State Tracking
D Chen, K Qian, Z Yu - arXiv preprint arXiv:2302.05932, 2023
Prompt-based methods with large pre-trained language models (PLMs) have shown
impressive unaided performance across many NLP tasks. These models improve
even further with the addition of a few labeled in-context exemplars to guide output …
저장	Twitter	LinkedIn	Facebook

[PDF] Decoupling the Skeleton Parsing and Schema Linking for Text-to-SQL
H Li, J Zhang, C Li, H Chen - arXiv preprint arXiv:2302.05965, 2023
One of the recent best attempts at Text-to-SQL is the pre-trained language model.
Due to the structural property of the SQL queries, the seq2seq model takes the
responsibility of parsing both the schema items (ie, tables and columns) and the …
저장	Twitter	LinkedIn	Facebook

[PDF] What do Language Models know about word senses? Zero-Shot WSD with Language Models and Domain Inventories
O Sainz, OL de Lacalle, E Agirre, G Rigau - arXiv preprint arXiv:2302.03353, 2023
Language Models are the core for almost any Natural Language Processing system
nowadays. One of their particularities is their contextualized representations, a game
changer feature when a disambiguation between word senses is necessary. In this …
저장	Twitter	LinkedIn	Facebook

[PDF] The Wisdom of Hindsight Makes Language Models Better Instruction Followers
T Zhang, F Liu, J Wong, P Abbeel, JE Gonzalez - arXiv preprint arXiv:2302.05206, 2023
Reinforcement learning has seen wide success in finetuning large language models
to better align with instructions via human feedback. The so-called algorithm,
Reinforcement Learning with Human Feedback (RLHF) demonstrates impressive …
저장	Twitter	LinkedIn	Facebook

[PDF] Task-Specific Skill Localization in Fine-tuned Language Models
A Panigrahi, N Saunshi, H Zhao, S Arora - arXiv preprint arXiv:2302.06600, 2023
Pre-trained language models can be fine-tuned to solve diverse NLP tasks, including
in few-shot settings. Thus fine-tuning allows the model to quickly pick up task-
specific skills,''but there has been limited study of where these newly-learnt skills …
저장	Twitter	LinkedIn	Facebook

[PDF] Discourse Structure Extraction from Pre-Trained and Fine-Tuned Language Models in Dialogues
C Li, P Huber, W Xiao, M Amblard, C Braud, G Carenini - arXiv preprint arXiv …, 2023
Discourse processing suffers from data sparsity, especially for dialogues. As a result,
we explore approaches to build discourse structures for dialogues, based on
attention matrices from Pre-trained Language Models (PLMs). We investigate …
저장	Twitter	LinkedIn	Facebook

[PDF] LongEval: Guidelines for Human Evaluation of Faithfulness in Long-form Summarization
K Krishna, E Bransom, B Kuehl, M Iyyer, P Dasigi… - arXiv preprint arXiv …, 2023
While human evaluation remains best practice for accurately judging the faithfulness
of automatically-generated summaries, few solutions exist to address the increased
difficulty and workload when evaluating long-form summaries. Through a survey of …
저장	Twitter	LinkedIn	Facebook

[PDF] Prompting Large Language Model for Machine Translation: A Case Study
B Zhang, B Haddow, A Birch - arXiv preprint arXiv:2301.07069, 2023
Research on prompting has shown excellent performance with little or even no
supervised training across many tasks. However, prompting for machine translation
is still under-explored in the literature. We fill this gap by offering a systematic study …
저장	Twitter	LinkedIn	Facebook

[PDF] Analyzing the Effectiveness of the Underlying Reasoning Tasks in Multi-hop Question Answering
X Ho, AKD Nguyen, S Sugawara, A Aizawa - arXiv preprint arXiv:2302.05963, 2023
To explain the predicted answers and evaluate the reasoning abilities of models,
several studies have utilized underlying reasoning (UR) tasks in multi-hop question
answering (QA) datasets. However, it remains an open question as to how effective …
저장	Twitter	LinkedIn	Facebook

[PDF] Selective In-Context Data Augmentation for Intent Detection using Pointwise V-Information
YT Lin, A Papangelis, S Kim, S Lee, D Hazarika… - arXiv preprint arXiv …, 2023
This work focuses on in-context data augmentation for intent detection. Having found
that augmentation via in-context prompting of large pre-trained language models
(PLMs) alone does not improve performance, we introduce a novel approach based …

[PDF] Knowledge is a Region in Weight Space for Fine-tuned Language Models
A Gueta, E Venezian, C Raffel, N Slonim, Y Katz… - arXiv preprint arXiv …, 2023
Research on neural networks has largely focused on understanding a single model
trained on a single dataset. However, relatively little is known about the relationships
between different models, especially those trained or tested on different datasets. We …
저장	Twitter	LinkedIn	Facebook

[PDF] MQAG: Multiple-choice Question Answering and Generation for Assessing Information Consistency in Summarization
P Manakul, A Liusie, MJF Gales - arXiv preprint arXiv:2301.12307, 2023
State-of-the-art summarization systems can generate highly fluent summaries. These
summaries, however, may contain factual inconsistencies and/or information not
present in the source. Hence, an important component of assessing the quality of …
저장	Twitter	LinkedIn	Facebook

[PDF] Probing Out-of-Distribution Robustness of Language Models with Parameter-Efficient Transfer Learning Methods
H Cho, C Park, J Kim, HJ Kim, KM Yoo, S Lee - arXiv preprint arXiv:2301.11660, 2023
As the size of the pre-trained language model (PLM) continues to increase,
numerous parameter-efficient transfer learning methods have been proposed
recently to compensate for the tremendous cost of fine-tuning. Despite the impressive …
저장	Twitter	LinkedIn	Facebook

[PDF] Progressive Prompts: Continual Learning for Language Models
A Razdaibiedina, Y Mao, R Hou, M Khabsa, M Lewis… - arXiv preprint arXiv …, 2023
We introduce Progressive Prompts-a simple and efficient approach for continual
learning in language models. Our method allows forward transfer and resists
catastrophic forgetting, without relying on data replay or a large number of task …
저장	Twitter	LinkedIn	Facebook

[PDF] Improved Knowledge Distillation for Pre-trained Language Models via Knowledge Selection
C Wang, Y Lu, Y Mu, Y Hu, T Xiao, J Zhu - arXiv preprint arXiv:2302.00444, 2023
Knowledge distillation addresses the problem of transferring knowledge from a
teacher model to a student model. In this process, we typically have multiple types of
knowledge extracted from the teacher model. The problem is to make full use of them …
저장	Twitter	LinkedIn	Facebook

Multi-stage transfer learning with BERTology-based language models for question answering system in vietnamese
K Van Nguyen, PNT Do, ND Nguyen, AGT Nguyen… - International Journal of …, 2023
With the fast growth of information science and engineering, a large number of
textual data generated are valuable for natural language processing and its
applications. Particularly, finding correct answers to natural language questions or …
저장	Twitter	LinkedIn	Facebook

[PDF] Debiased Fine-Tuning for Vision-language Models by Prompt Regularization
B Zhu, Y Niu, S Lee, M Hur, H Zhang - arXiv preprint arXiv:2301.12429, 2023
We present a new paradigm for fine-tuning large-scale visionlanguage pre-trained
models on downstream task, dubbed Prompt Regularization (ProReg). Different from
traditional fine-tuning which easily overfits to the downstream task data, ProReg uses …
저장	Twitter	LinkedIn	Facebook

[PDF] Understanding Finetuning for Factual Knowledge Extraction from Language Models
M Kazemi, S Mittal, D Ramachandran - arXiv preprint arXiv:2301.11293, 2023
Language models (LMs) pretrained on large corpora of text from the web have been
observed to contain large amounts of various types of knowledge about the world.
This observation has led to a new and exciting paradigm in knowledge graph …
저장	Twitter	LinkedIn	Facebook

[PDF] Few-Shot Table-to-Text Generation with Prompt Planning and Knowledge Memorization
Z Guo, M Yan, J Qi, J Zhou, Z He, Z Lin, G Zheng… - arXiv preprint arXiv …, 2023
Pre-trained language models (PLM) have achieved remarkable advancement in
table-to-text generation tasks. However, the lack of labeled domain-specific
knowledge and the topology gap between tabular data and text make it difficult for …
저장	Twitter	LinkedIn	Facebook

[PDF] Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning
MD Ma, JY Kao, S Gao, A Gupta, D Jin, T Chung… - arXiv preprint arXiv …, 2023
Dialogue state tracking (DST) is an important step in dialogue management to keep
track of users' beliefs. Existing works fine-tune all language model (LM) parameters
to tackle the DST task, which requires significant data and computing resources for …
저장	Twitter	LinkedIn	Facebook

### Jianfeng Gao님의 자료를 팔로우하세요
Grounded language-image pre-training
LH Li, P Zhang, H Zhang, J Yang, C Li, Y Zhong, L Wang, L Yuan, L Zhang, JN …
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern …, 2022

Vinvl: Revisiting visual representations in vision-language models
P Zhang, X Li, X Hu, J Yang, L Zhang, L Wang, Y Choi, J Gao
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern …, 2021

Unified vision-language pre-training for image captioning and vqa
L Zhou, H Palangi, L Zhang, H Hu, J Corso, J Gao
Proceedings of the AAAI conference on artificial intelligence, 2020

- []()
- []()
